"""
Agent NLP MCP avec capacit√©s IA compl√®tes - Version Agent Autonome
=================================================================

Agent IA autonome MCP qui peut :
- Analyser des files et d√©tecter les PII avec intelligence contextuelle
- Raisonner sur les t√¢ches √† accomplir (ReAct pattern)
- Planifier ses actions de mani√®re autonome
- Utiliser des outils de mani√®re intelligente
- G√©rer des demandes complexes avec Ollama/Llama
- Exposer toutes ses capacit√©s via le protocole MCP officiel

Utilise Ollama/Llama + LangChain pour l'IA, avec fallback intelligent.
"""

import asyncio
import json
import logging
import os
import re
import sys
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from collections import Counter

# Suppression des avertissements Pydantic
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Imports MCP officiels
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp import Context
from mcp.server.fastmcp import Image
from mcp.types import TextContent
from pydantic import BaseModel, Field

# LangChain imports pour l'IA
try:
    from langchain_community.llms import Ollama
    from langchain.tools import tool
    LANGCHAIN_AVAILABLE = True
    print("‚úÖ LangChain disponible")
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("‚ö†Ô∏è LangChain non disponible")

# Support PDF
try:
    import PyPDF2
    PDF_AVAILABLE = True
    print("‚úÖ Support PDF disponible (PyPDF2)")
except ImportError:
    PDF_AVAILABLE = False
    print("‚ö†Ô∏è Support PDF non disponible (pip install PyPDF2)")

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    print("‚úÖ Support PDF avanc√© disponible (PyMuPDF)")
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("‚ö†Ô∏è Support PDF avanc√© non disponible (pip install pymupdf)")

# Configuration du logging avec support Unicode pour Windows
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Configuration Ollama
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
LLAMA_MODEL = os.getenv("LLAMA_MODEL", "llama3.2:latest")

# Initialisation du LLM
llm = None
if LANGCHAIN_AVAILABLE:
    try:
        llm = Ollama(
            model=LLAMA_MODEL,
            base_url=OLLAMA_BASE_URL,
            temperature=0.7
        )
        # Test de connexion
        test_response = llm.invoke("Test")
        logger.info("‚úÖ Ollama/Llama connect√© avec succ√®s")
        print("‚úÖ Agent IA Ollama/Llama pr√™t")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Ollama connection failed: {e}")
        print(f"‚ö†Ô∏è Ollama connection failed: {e}")
        llm = None

# Regex patterns avanc√©s pour la d√©tection PII
PII_REGEXES: Dict[str, re.Pattern] = {
    "EMAIL": re.compile(r"[\w\.\-]+@[\w\.-]+\.[a-z]{2,}", re.I),
    "PHONE": re.compile(r"\+?\d{1,3}[\s\-]?\d[\d\s\-]{8,}\d", re.I),
    "CARD": re.compile(r"\b\d{4}[\s\-]?\d{4}[\s\-]?\d{4}[\s\-]?\d{4}\b"),
    "IBAN": re.compile(r"\b[A-Z]{2}\d{2}[\s]?[A-Z0-9]{4}[\s]?\d{4}[\s]?\d{4}[\s]?\d{4}[\s]?\d{4}[\s]?\d{2}\b"),
    "SSN": re.compile(r"\b\d{3}-?\d{2}-?\d{4}\b"),
}
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Fonctions utilitaires pour le traitement de files (identiques √† l'original)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_pdf_content(file_path: str) -> str:
    """
    Extrait le contenu texte d'un fichier PDF.
    
    Args:
        file_path: Chemin vers le fichier PDF
        
    Returns:
        Contenu texte du PDF
    """
    content = ""
    
    # Essai avec PyMuPDF d'abord (plus performant)
    if PYMUPDF_AVAILABLE:
        try:
            doc = fitz.open(file_path)
            for page_num in range(doc.page_count):
                page = doc[page_num]
                content += page.get_text() + "\n"
            doc.close()
            return content
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur PyMuPDF: {e}")
    
    # Fallback avec PyPDF2
    if PDF_AVAILABLE:
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    content += page.extract_text() + "\n"
            return content
        except Exception as e:
            return f"Erreur lors de l'extraction PDF: {str(e)}"
    
    return "Aucune biblioth√®que PDF disponible"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Configuration et mod√®les avanc√©s (mis √† jour avec les capacit√©s de l'original)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@dataclass
class NLPConfig:
    """Configuration for NLP agent with AI"""
    max_text_length: int = 10000
    use_ai_analysis: bool = True  # Use AI for analysis
    llm_model: str = LLAMA_MODEL
    log_file: str = str(Path(__file__).parent.parent / "logs" / "nlp_agent.log")
    offline_mode: bool = False

# Mod√®les Pydantic pour la sortie structur√©e MCP
class FileAnalysisResult(BaseModel):
    """R√©sultat de l'analyse de fichier (compatible avec l'original)"""
    file_path: str = Field(description="Chemin du fichier analys√©")
    resume: str = Field(description="R√©sum√© intelligent du contenu")
    warning: bool = Field(description="Pr√©sence d'informations PII")
    analysis_method: str = Field(description="M√©thode d'analyse utilis√©e", default="ai_enhanced")
    
class TextAnalysisResult(BaseModel):
    """R√©sultat de l'analyse de texte"""
    word_count: int = Field(description="Nombre de mots")
    char_count: int = Field(description="Nombre de caract√®res")
    sentences: int = Field(description="Nombre de phrases")
    paragraphs: int = Field(description="Nombre de paragraphes")
    keywords: List[str] = Field(description="Mots-cl√©s principaux")
    sentiment: str = Field(description="Sentiment g√©n√©ral")
    language: str = Field(description="Langue d√©tect√©e")
    ai_summary: Optional[str] = Field(description="R√©sum√© g√©n√©r√© par l'IA", default=None)

class PIIDetectionResult(BaseModel):
    """R√©sultat de la d√©tection PII"""
    found_pii: bool = Field(description="PII d√©tect√©es")
    pii_types: List[str] = Field(description="Types de PII trouv√©s")
    pii_count: int = Field(description="Nombre total de PII")
    redacted_text: str = Field(description="Texte avec PII masqu√©es")
    ai_analysis: Optional[str] = Field(description="Analyse IA des PII", default=None)

class SummaryResult(BaseModel):
    """R√©sultat du r√©sum√©"""
    summary: str = Field(description="R√©sum√© du texte")
    key_points: List[str] = Field(description="Points cl√©s")
    summary_length: int = Field(description="Longueur du r√©sum√©")
    compression_ratio: float = Field(description="Ratio de compression")
    ai_enhanced: bool = Field(description="R√©sum√© am√©lior√© par l'IA", default=False)

class TranslationResult(BaseModel):
    """R√©sultat de la traduction"""
    translated_text: str = Field(description="Texte traduit")
    source_language: str = Field(description="Langue source")
    target_language: str = Field(description="Langue cible")
    confidence: float = Field(description="Confiance de la traduction")

class BatchProcessingResult(BaseModel):
    """R√©sultat du traitement en lot"""
    batch_info: Dict[str, Any] = Field(description="Informations du lot")
    files: List[Dict[str, Any]] = Field(description="R√©sum√© des files")
    detailed_results: Dict[str, Any] = Field(description="R√©sultats d√©taill√©s")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Agent IA Principal (identique √† l'original avec int√©gration MCP)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class NLPAgent:
    """Agent IA autonome pour l'analyse NLP avec capacit√© de raisonnement - Version MCP"""
    
    def __init__(self, config: NLPConfig):
        self.config = config
        self.llm = llm if not config.offline_mode else None
        self.conversation_history = []  # M√©moire simple
        self.setup_logging()
        
    def setup_logging(self):
        """Configuration du logging"""
        os.makedirs(os.path.dirname(self.config.log_file), exist_ok=True)
        handler = logging.FileHandler(self.config.log_file)
        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(handler)
        
    def _generate_smart_summary(self, content: str) -> str:
        """G√©n√®re un r√©sum√© intelligent avec LLM si disponible (identique √† l'original)"""
        if not self.config.offline_mode and self.llm:
            try:
                prompt = f"""R√©sume le texte suivant en 3 phrases maximum en fran√ßais, en gardant les informations essentielles :

Texte : {content}

R√©sum√© :
Ne commence jamais le r√©sum√© par une introduction de type "Voici le r√©sum√©" ou similaire.
"""
                response = self.llm.invoke(prompt)
                return response.strip()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur LLM, passage en mode offline : {e}")
        
        # Fallback : r√©sum√© simple
        sentences = content.replace('\n', ' ').split('.')
        summary_sentences = []
        for sentence in sentences:
            if sentence.strip() and len(summary_sentences) < 3:
                summary_sentences.append(sentence.strip())
        
        return '. '.join(summary_sentences) + '.' if summary_sentences else "R√©sum√© non disponible."

    def _detect_pii_intelligent(self, content: str) -> bool:
        """D√©tecte les PII avec IA si disponible, sinon utilise regex (identique √† l'original)"""
        if not self.config.offline_mode and self.llm:
            try:
                prompt = f"""Tu es un expert en s√©curit√© des donn√©es. Analyse le texte suivant et d√©tecte UNIQUEMENT les informations personnelles identifiables (PII) r√©elles pr√©sentes.

Types de PII √† rechercher avec attention au contexte:
- Adresses email r√©elles (pas d'exemples fictifs)
- Num√©ros de t√©l√©phone r√©els
- Num√©ros de carte bancaire/cr√©dit r√©els (pas 4242 4242 4242 4242)
- Codes IBAN/RIB r√©els
- Num√©ros de s√©curit√© sociale r√©els
- Adresses postales compl√®tes r√©elles
- Dates de naissance sp√©cifiques
- Num√©ros d'identit√©/passeport r√©els
- Noms et pr√©noms complets de personnes r√©elles

IMPORTANT: Ignore les exemples fictifs, les donn√©es de test, les placeholders, et les donn√©es manifestement fausses.

Texte √† analyser :
{content}

R√©ponds UNIQUEMENT par:
- "NONE_PII" si aucune information personnelle r√©elle n'est d√©tect√©e
- "PII_DETECTEES" si des PII r√©elles sont pr√©sentes

R√©ponse:"""
                
                response = self.llm.invoke(prompt)
                response = response.strip().upper()
                
                if "PII_DETECTEES" in response:
                    logger.info("[PII] PII d√©tect√©es par l'IA")
                    return True
                else:
                    logger.info("[OK] Aucune PII d√©tect√©e par l'IA")
                    return False
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur LLM pour PII, passage en mode regex : {e}")
        
        # Fallback : d√©tection par regex
        for label, pattern in PII_REGEXES.items():
            if pattern.search(content):
                logger.info(f"[PII] PII d√©tect√©es par regex: {label}")
                return True
        
        logger.info("[OK] Aucune PII d√©tect√©e par regex")
        return False
    
    def reason_and_act(self, query: str) -> str:
        """Raisonne et agit sur une requ√™te donn√©e (identique √† l'original)"""
        if not self.llm or self.config.offline_mode:
            return "Agent IA non disponible, utilisation du mode fallback"
        
        # Template de raisonnement ReAct simplifi√©
        prompt = f"""Tu es un agent IA sp√©cialis√© en analyse NLP et d√©tection PII.

Tu as acc√®s aux outils suivants:
1. read_file_tool(file_path) - Lit un fichier (texte ou PDF)
2. generate_smart_summary_tool(text) - G√©n√®re un r√©sum√© intelligent
3. detect_pii_tool(text) - D√©tecte les PII
4. save_json_tool(data, filename) - Sauvegarde en JSON
5. list_files_tool(directory) - Liste les files
6. process_multiple_files_tool(file_paths) - Traite plusieurs files (s√©par√©s par des virgules)

Utilise le format de raisonnement suivant:
Thought: [Ce que je dois faire]
Action: [Outil √† utiliser]
Action Input: [Param√®tres de l'outil]
Observation: [R√©sultat de l'action]
... (r√©p√®te si n√©cessaire)
Final Answer: [R√©ponse finale]

Question: {query}

Commence par r√©fl√©chir:"""

        try:
            # G√©n√©ration de la r√©ponse avec raisonnement
            response = self.llm.invoke(prompt)
            
            # Traitement de la r√©ponse pour extraire les actions
            return self._process_reasoning_response(response, query)
        except Exception as e:
            return f"Erreur lors du raisonnement: {e}"
    
    def _process_reasoning_response(self, response: str, original_query: str) -> str:
        """Traite la r√©ponse de raisonnement et ex√©cute les actions (identique √† l'original)"""
        lines = response.split('\n')
        result = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('Action:'):
                action = line.replace('Action:', '').strip()
                result.append(f"[ACTION] Action: {action}")
            elif line.startswith('Thought:'):
                thought = line.replace('Thought:', '').strip()
                result.append(f"[THOUGHT] Reflexion: {thought}")
            elif line.startswith('Final Answer:'):
                answer = line.replace('Final Answer:', '').strip()
                result.append(f"[ANSWER] Reponse finale: {answer}")
        
        return '\n'.join(result) if result else response
    
    def process_file_with_reasoning(self, file_path: str) -> Dict[str, Any]:
        """Traite un fichier en utilisant le raisonnement de l'agent IA (identique √† l'original)"""
        
        if self.llm and not self.config.offline_mode:
            try:
                # Utilisation de l'agent IA pour traiter le fichier
                query = f"""Analyse le fichier '{file_path}' et produis un r√©sultat JSON avec:
                1. Lis le contenu du fichier
                2. G√©n√®re un r√©sum√© du contenu
                3. D√©tecte la pr√©sence de PII
                4. Cr√©e un JSON avec file_path, resume, et warning (boolean)
                5. Sauvegarde le r√©sultat dans un fichier nomm√© '{os.path.splitext(os.path.basename(file_path))[0]}_analysis.json'
                
                Retourne uniquement le JSON final."""
                
                # Raisonnement de l'agent
                reasoning = self.reason_and_act(query)
                logger.info(f"[AI] Raisonnement de l'agent:\n{reasoning}\n")
                
                # Ex√©cution directe des outils pour obtenir le r√©sultat
                return self._execute_analysis(file_path)
                
            except Exception as e:
                logger.error(f"Erreur avec l'agent IA: {e}")
                return self._fallback_process_file(file_path)
        else:
            return self._fallback_process_file(file_path)
    
    def _execute_analysis(self, file_path: str) -> Dict[str, Any]:
        """Ex√©cute l'analyse avec les outils disponibles (identique √† l'original)"""
        try:
            # Lecture du fichier directement
            logger.info("[READ] Lecture du fichier...")
            if not os.path.isabs(file_path):
                current_dir = os.getcwd()
                full_path = os.path.join(current_dir, file_path)
            else:
                full_path = file_path
            
            # V√©rifier l'extension du fichier
            _, ext = os.path.splitext(full_path.lower())
            
            if ext == '.pdf':
                # Traitement PDF
                if PDF_AVAILABLE or PYMUPDF_AVAILABLE:
                    content = extract_pdf_content(full_path)
                    logger.info(f"[PDF] Contenu PDF extrait ({len(content)} caract√®res)")
                else:
                    content = "Erreur: Aucune biblioth√®que PDF disponible"
                    logger.warning("[WARN] Aucune biblioth√®que PDF disponible")
            else:
                # Traitement files texte
                with open(full_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                logger.info(f"[TEXT] Contenu texte lu ({len(content)} caract√®res)")
            
            # G√©n√©ration du r√©sum√© intelligent
            logger.info("[AI] G√©n√©ration du r√©sum√©...")
            resume = self._generate_smart_summary(content)
            
            # D√©tection PII intelligente
            logger.info("[AI] D√©tection intelligente des PII...")
            pii_found = self._detect_pii_intelligent(content)
            
            # Construction du r√©sultat final
            result = {
                "file_path": os.path.abspath(file_path),
                "resume": resume,
                "warning": pii_found
            }
            
            # Sauvegarde automatique
            output_file = f"{os.path.splitext(os.path.basename(file_path))[0]}_analysis.json"
            logger.info(f"[SAVE] Sauvegarde dans {output_file}...")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            return result
            
        except Exception as e:
            return {
                "file_path": os.path.abspath(file_path) if os.path.exists(file_path) else file_path,
                "resume": f"Erreur lors du traitement : {str(e)}",
                "warning": False
            }
    
    def _fallback_process_file(self, file_path: str) -> Dict[str, Any]:
        """Traitement de fichier en mode fallback (sans agent IA) - identique √† l'original"""
        try:
            # Lecture du fichier directement
            logger.info("[READ] Lecture du fichier...")
            if not os.path.isabs(file_path):
                current_dir = os.getcwd()
                full_path = os.path.join(current_dir, file_path)
            else:
                full_path = file_path
            
            # V√©rifier l'extension du fichier
            _, ext = os.path.splitext(full_path.lower())
            
            if ext == '.pdf':
                # Traitement PDF
                if PDF_AVAILABLE or PYMUPDF_AVAILABLE:
                    content = extract_pdf_content(full_path)
                    logger.info(f"[PDF] Contenu PDF extrait ({len(content)} caract√®res)")
                else:
                    content = "Erreur: Aucune biblioth√®que PDF disponible"
                    logger.warning("[WARN] Aucune biblioth√®que PDF disponible")
            else:
                # Traitement files texte
                with open(full_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                logger.info(f"[TEXT] Contenu texte lu ({len(content)} caract√®res)")
            
            # G√©n√©ration du r√©sum√© intelligent
            logger.info("[AI] G√©n√©ration du r√©sum√©...")
            resume = self._generate_smart_summary(content)
            
            # D√©tection PII intelligente
            logger.info("[AI] D√©tection intelligente des PII...")
            pii_found = self._detect_pii_intelligent(content)
            
            # Construction du r√©sultat final
            result = {
                "file_path": os.path.abspath(file_path),
                "resume": resume,
                "warning": pii_found
            }
            
            return result
            
        except Exception as e:
            return {
                "file_path": os.path.abspath(file_path) if os.path.exists(file_path) else file_path,
                "resume": f"Erreur lors du traitement : {str(e)}",
                "warning": False
            }
    
    def chat(self, message: str) -> str:
        """Interface de chat avec l'agent IA (identique √† l'original)"""
        if self.llm and not self.config.offline_mode:
            try:
                return self.reason_and_act(message)
            except Exception as e:
                return f"Erreur lors du chat: {e}"
        else:
            return "Mode offline activ√©. Utilisez les commandes directes pour traiter les files."
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extraction de texte depuis un PDF"""
        try:
            return extract_pdf_content(pdf_path)
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction PDF: {e}")
            return ""
    
    def generate_smart_summary(self, text: str) -> str:
        """G√©n√®re un r√©sum√© intelligent avec LLM si disponible, sinon utilise un fallback."""
        return self._generate_smart_summary(text)

    def detect_pii_intelligent(self, text: str) -> PIIDetectionResult:
        """D√©tecte les informations personnelles identifiables (PII) dans un texte avec IA."""
        ai_analysis = None
        found_pii = False
        pii_types = []
        pii_count = 0
        redacted_text = text
        
        try:
            # Utilisation du LLM si disponible pour une d√©tection intelligente
            if self.llm and self.config.use_ai_analysis:
                try:
                    prompt = f"""Tu es un expert en s√©curit√© des donn√©es. Analyse le texte suivant et d√©tecte UNIQUEMENT les informations personnelles identifiables (PII) r√©elles pr√©sentes.

Types de PII √† rechercher avec attention au contexte:
- Adresses email r√©elles (pas d'exemples fictifs)
- Num√©ros de t√©l√©phone r√©els
- Num√©ros de carte bancaire/cr√©dit r√©els (pas 4242 4242 4242 4242)
- Codes IBAN/RIB r√©els
- Num√©ros de s√©curit√© sociale r√©els
- Adresses postales compl√®tes r√©elles
- Dates de naissance sp√©cifiques
- Num√©ros d'identit√©/passeport r√©els
- Noms et pr√©noms complets de personnes r√©elles

IMPORTANT: Ignore les exemples fictifs, les donn√©es de test, les placeholders, et les donn√©es manifestement fausses.

Texte √† analyser :
{text}

R√©ponds UNIQUEMENT par:
- "NONE_PII" si aucune information personnelle r√©elle n'est d√©tect√©e
- "PII_DETECTEES: [types d√©tect√©s]" si des PII r√©elles sont pr√©sentes

R√©ponse:"""
                    
                    response = self.llm.invoke(prompt)
                    ai_analysis = response.strip()
                    
                    if "NONE_PII" in response.upper():
                        found_pii = False
                    elif "PII_DETECTEES" in response.upper():
                        found_pii = True
                        # Extraire les types si sp√©cifi√©s
                        if ":" in response:
                            types_part = response.split(":")[1].strip()
                            pii_types = [t.strip() for t in types_part.split(",")]
                        
                except Exception as llm_error:
                    logger.warning(f"‚ö†Ô∏è Erreur LLM pour PII, passage en mode regex : {llm_error}")
            
            # Fallback ou compl√©ment : d√©tection par regex
            if not ai_analysis or not found_pii:
                regex_pii = []
                
                for label, pattern in PII_REGEXES.items():
                    matches = pattern.findall(text)
                    if matches:
                        found_pii = True
                        regex_pii.append(label)
                        pii_count += len(matches)
                        # Masquer les PII
                        redacted_text = pattern.sub(f"[{label}_REDACTED]", redacted_text)
                
                if regex_pii:
                    pii_types.extend(regex_pii)
            
            pii_count = max(pii_count, len(pii_types))
            
            return PIIDetectionResult(
                found_pii=found_pii,
                pii_types=list(set(pii_types)),  # Supprimer les doublons
                pii_count=pii_count,
                redacted_text=redacted_text,
                ai_analysis=ai_analysis
            )
            
        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection PII: {e}")
            return PIIDetectionResult(
                found_pii=False,
                pii_types=[],
                pii_count=0,
                redacted_text=text,
                ai_analysis=f"Erreur: {str(e)}"
            )
            
    def analyze_text_basic(self, text: str) -> TextAnalysisResult:
        """Analyse basique du texte avec am√©lioration IA"""
        words = text.split()
        sentences = text.split('.')
        paragraphs = text.split('\n\n')
        
        # Extraction des mots-cl√©s simples
        word_freq = Counter(word.lower().strip('.,!?;:') for word in words if len(word) > 3)
        keywords = [word for word, freq in word_freq.most_common(10)]
        
        # D√©tection de langue simple
        language = "fr" if any(word in text.lower() for word in ["le", "la", "de", "et", "√†"]) else "en"
        
        # Sentiment par d√©faut
        sentiment = "neutral"
        ai_summary = None
        
        # Utilisation de l'IA si disponible
        if self.llm and self.config.use_ai_analysis:
            try:
                # Analyse du sentiment avec l'IA
                sentiment_prompt = f"""Analyse le sentiment du texte suivant et r√©ponds UNIQUEMENT par: positif, n√©gatif, ou neutre.

Texte: {text[:500]}

Sentiment:"""
                sentiment_response = self.llm.invoke(sentiment_prompt)
                sentiment = sentiment_response.strip().lower()
                
                # G√©n√©ration d'un r√©sum√© IA
                ai_summary = self.generate_smart_summary(text)
                
            except Exception as e:
                logger.warning(f"Erreur analyse IA: {e}")
        
        return TextAnalysisResult(
            word_count=len(words),
            char_count=len(text),
            sentences=len(sentences),
            paragraphs=len(paragraphs),
            keywords=keywords,
            sentiment=sentiment,
            language=language,
            ai_summary=ai_summary
        )
        
    def summarize_text(self, text: str, max_length: int = 150) -> SummaryResult:
        """R√©sum√© du texte avec IA"""
        # Utiliser l'IA pour le r√©sum√© si disponible
        ai_enhanced = False
        summary = ""
        key_points = []
        
        if self.llm and self.config.use_ai_analysis:
            try:
                summary = self.generate_smart_summary(text)
                ai_enhanced = True
                
                # Extraction des points cl√©s avec l'IA
                key_points_prompt = f"""Extrais 3 points cl√©s maximum du texte suivant sous forme de liste courte:

Texte: {text}

Points cl√©s (un par ligne, commence par -):"""
                key_points_response = self.llm.invoke(key_points_prompt)
                key_points = [point.strip('- ').strip() for point in key_points_response.split('\n') if point.strip().startswith('-')][:3]
                
            except Exception as e:
                logger.warning(f"Erreur r√©sum√© IA: {e}")
                ai_enhanced = False
        
        # Fallback si l'IA n'est pas disponible
        if not summary:
            sentences = text.split('.')
            if len(sentences) <= 3:
                summary = text
            else:
                # Prendre les premi√®res et derni√®res phrases
                summary = '. '.join(sentences[:2] + sentences[-1:])
            
            # Points cl√©s basiques
            if len(sentences) > 5:
                key_points = sentences[1:4]  # Phrases du milieu
        
        compression_ratio = len(summary) / len(text) if text else 0
        
        return SummaryResult(
            summary=summary,
            key_points=key_points,
            summary_length=len(summary),
            compression_ratio=compression_ratio,
            ai_enhanced=ai_enhanced
        )
        
    def translate_text(self, text: str, target_lang: str = "en") -> TranslationResult:
        """Traduction avec IA si disponible"""
        # Utiliser l'IA pour la traduction si disponible
        if self.llm and self.config.use_ai_analysis:
            try:
                translate_prompt = f"""Traduis le texte suivant en {target_lang}. R√©ponds UNIQUEMENT avec la traduction, sans commentaire:

Texte √† traduire: {text}

Traduction:"""
                translated_text = self.llm.invoke(translate_prompt).strip()
                
                return TranslationResult(
                    translated_text=translated_text,
                    source_language="auto",
                    target_language=target_lang,
                    confidence=0.9
                )
            except Exception as e:
                logger.warning(f"Erreur traduction IA: {e}")
        
        # Fallback : traduction basique (placeholder)
        return TranslationResult(
            translated_text=f"[TRANSLATION to {target_lang}]: {text}",
            source_language="auto",
            target_language=target_lang,
            confidence=0.8
        )
        
    def process_multiple_files_with_reasoning(self, file_paths: List[str]) -> Dict[str, Any]:
        """Traite plusieurs files en utilisant le raisonnement de l'agent IA (identique √† l'original)"""
        results = {}
        summary_results = []
        total_warnings = 0
        
        logger.info(f"[BATCH] Traitement de {len(file_paths)} files...")
        
        for i, file_path in enumerate(file_paths, 1):
            logger.info(f"[{i}/{len(file_paths)}] Traitement du fichier: {file_path}")
            
            try:
                # Traiter chaque fichier individuellement
                result = self.process_file_with_reasoning(file_path)
                
                # Ajouter au r√©sultat global
                file_key = os.path.basename(file_path)
                results[file_key] = result
                
                # Pr√©parer le r√©sum√©
                summary_results.append({
                    "file": file_key,
                    "path": result["file_path"],
                    "has_pii": result["warning"],
                    "summary_preview": result["resume"][:100] + "..." if len(result["resume"]) > 100 else result["resume"]
                })
                
                if result["warning"]:
                    total_warnings += 1
                    
            except Exception as e:
                logger.error(f"[ERROR] Erreur lors du traitement de {file_path}: {e}")
                results[os.path.basename(file_path)] = {
                    "file_path": file_path,
                    "resume": f"Erreur lors du traitement: {str(e)}",
                    "warning": False
                }
        
        # Cr√©er un r√©sum√© global
        batch_summary = {
            "batch_info": {
                "total_files": len(file_paths),
                "processed_files": len(results),
                "files_with_pii": total_warnings,
                "processing_date": datetime.now().isoformat()
            },
            "files": summary_results,
            "detailed_results": results
        }
        
        return batch_summary

# Cr√©ation de l'agent NLP avec configuration
config = NLPConfig()
agent = None  # Lazy loading

def get_agent():
    """Lazy loading de l'agent NLP"""
    global agent
    if agent is None:
        agent = NLPAgent(config)
    return agent

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Serveur MCP avec capacit√©s IA compl√®tes
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Cr√©ation du serveur MCP
mcp = FastMCP("NLP Agent IA", dependencies=["langchain_community", "PyPDF2", "pydantic", "pymupdf"])

# Ressources MCP am√©lior√©es
@mcp.resource("nlp://config")
def get_nlp_config() -> str:
    """Configuration de l'agent NLP avec IA"""
    return json.dumps({
        "max_text_length": config.max_text_length,
        "use_ai_analysis": config.use_ai_analysis,
        "llm_model": config.llm_model,
        "supported_formats": ["txt", "pdf", "py", "md", "json", "csv", "xml", "html", "log"],
        "capabilities": ["analyze", "summarize", "detect_pii", "translate", "reason", "chat", "batch_processing"],
        "ai_features": [
            "intelligent_summary", 
            "contextual_pii_detection", 
            "sentiment_analysis", 
            "reasoning_and_planning",
            "autonomous_task_execution",
            "multi_format_support",
            "batch_processing"
        ],
        "ollama_status": "connected" if llm else "disconnected",
        "fallback_mode": "available"
    }, indent=2)

@mcp.resource("nlp://status")
def get_nlp_status() -> str:
    """Statut complet de l'agent NLP IA"""
    return json.dumps({
        "status": "ready",
        "ai_available": llm is not None,
        "agent_mode": "ai_enhanced" if llm else "fallback",
        "langchain_available": LANGCHAIN_AVAILABLE,
        "pdf_support": PDF_AVAILABLE or PYMUPDF_AVAILABLE,
        "pymupdf_available": PYMUPDF_AVAILABLE,
        "pypdf2_available": PDF_AVAILABLE,
        "offline_mode": config.offline_mode,
        "timestamp": datetime.now().isoformat(),
        "version": "3.0.0 - Agent IA Autonome MCP",
        "reasoning_capabilities": "ReAct pattern enabled" if llm else "disabled",
        "supported_operations": [
            "file_analysis",
            "text_analysis", 
            "pii_detection",
            "summarization",
            "translation",
            "chat_interaction",
            "reasoning_and_planning",
            "batch_processing"
        ]
    }, indent=2)

@mcp.resource("nlp://capabilities")
def get_nlp_capabilities() -> str:
    """Capacit√©s avanc√©es de l'agent IA"""
    return json.dumps({
        "autonomous_features": {
            "reasoning": "ReAct pattern with Llama 3.2",
            "planning": "Multi-step task orchestration",
            "memory": "Conversation history maintained",
            "fallback": "Intelligent degradation without AI"
        },
        "analysis_capabilities": {
            "text_analysis": "Deep NLP analysis with AI enhancement",
            "sentiment_analysis": "AI-powered sentiment detection",
            "keyword_extraction": "Frequency-based + AI semantic analysis",
            "language_detection": "Rule-based + AI confirmation"
        },
        "pii_detection": {
            "method": "AI + Regex hybrid approach",
            "context_awareness": "Distinguishes real vs. fake data",
            "supported_types": ["email", "phone", "credit_card", "ssn", "iban", "addresses"]
        },
        "document_processing": {
            "formats": ["PDF", "TXT", "MD", "JSON", "CSV", "XML", "HTML", "LOG", "PY"],
            "pdf_extraction": "PyMuPDF + PyPDF2 fallback",
            "batch_processing": "Multiple files simultaneously"
        },
        "ai_integration": {
            "llm_model": LLAMA_MODEL,
            "base_url": OLLAMA_BASE_URL,
            "temperature": 0.7,
            "connection_status": "active" if llm else "inactive"
        }
    }, indent=2)

# Outils MCP avec capacit√©s IA compl√®tes (identiques √† l'agent original)
@mcp.tool()
def analyze_text(text: str) -> TextAnalysisResult:
    """Analyse compl√®te d'un texte avec IA (identique √† l'agent original)"""
    if len(text) > config.max_text_length:
        text = text[:config.max_text_length]
    
    try:
        result = get_agent().analyze_text_basic(text)
        logger.info(f"Analyse de texte r√©ussie: {result.word_count} mots, IA: {result.ai_summary is not None}")
        return result
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse: {e}")
        raise

@mcp.tool()
def analyze_file(file_path: str) -> FileAnalysisResult:
    """Analyse d'un fichier avec IA (compatibilit√© avec l'agent original)"""
    try:
        result = get_agent().process_file_with_reasoning(file_path)
        
        # Conversion vers le format MCP
        return FileAnalysisResult(
            file_path=result["file_path"],
            resume=result["resume"],
            warning=result["warning"],
            analysis_method="ai_enhanced" if llm else "fallback"
        )
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse du fichier {file_path}: {e}")
        raise

@mcp.tool()

def analyze_text_pii_with_ai_corrected(text: str) -> Dict[str, Any]:
    """
    Version corrig√©e de l'analyse PII avec IA + fallback regex FORC√â
    
    CORRECTION CRITIQUE:
    - Force l'utilisation du fallback regex si l'IA √©choue
    - Combine les r√©sultats IA + regex pour plus de robustesse
    - Log d√©taill√© pour debugging
    - Logique de d√©cision am√©lior√©e
    """
    logger.info("üîç Analyse PII corrig√©e - D√©marrage")
    
    # R√©sultat par d√©faut
    result = {
        "filepath": "analysis",
        "summary": "Analyse en cours...",
        "warning": False,
        "pii_detected": [],
        "method_used": "unknown",
        "debug_info": {}
    }
    
    # 1. TOUJOURS faire l'analyse regex en premier (fallback garanti)
    logger.info("üîç √âtape 1: Analyse regex (fallback)")
    regex_pii = detect_pii_in_text(text)
    regex_warning = len(regex_pii) > 0
    
    result["debug_info"]["regex_pii"] = regex_pii
    result["debug_info"]["regex_warning"] = regex_warning
    
    logger.info(f"üìä Regex - PII d√©tect√©es: {regex_pii}")
    logger.info(f"üìä Regex - Warning: {regex_warning}")
    
    # 2. Tentative d'analyse IA (si disponible)
    ai_result = None
    ai_warning = False
    
    if llm and LANGCHAIN_AVAILABLE:
        try:
            logger.info("üîç √âtape 2: Analyse IA")
            
            prompt = f"""Analyse ce texte et d√©termine s'il contient des informations personnelles identifiables (PII).
            
PII incluent: emails, t√©l√©phones, cartes bancaires, IBAN, adresses, noms complets, etc.

Texte √† analyser:
{text[:1000]}

R√©ponds par 'OUI' si PII d√©tect√©es, 'NON' sinon, suivi d'une explication courte."""

            ai_response = llm.invoke(prompt)
            
            # Parser la r√©ponse IA
            if ai_response and isinstance(ai_response, str):
                ai_response_lower = ai_response.lower()
                if "oui" in ai_response_lower or "yes" in ai_response_lower:
                    ai_warning = True
                elif "non" in ai_response_lower or "no" in ai_response_lower:
                    ai_warning = False
                else:
                    # R√©ponse ambigu√´, utiliser regex
                    logger.warning("‚ö†Ô∏è R√©ponse IA ambigu√´, utilisation regex")
                    ai_warning = regex_warning
                
                ai_result = ai_response
                result["debug_info"]["ai_response"] = ai_response
                result["debug_info"]["ai_warning"] = ai_warning
                
                logger.info(f"üìä IA - R√©ponse: {ai_response[:100]}...")
                logger.info(f"üìä IA - Warning: {ai_warning}")
            else:
                logger.warning("‚ö†Ô∏è R√©ponse IA invalide, utilisation regex")
                ai_warning = regex_warning
                
        except Exception as e:
            logger.error(f"‚ùå Erreur IA: {e}")
            ai_result = None
            ai_warning = regex_warning  # Fallback vers regex
    else:
        logger.info("‚ÑπÔ∏è IA non disponible, utilisation regex uniquement")
        ai_warning = regex_warning
    
    # 3. LOGIQUE DE D√âCISION CORRIG√âE
    # Priorit√©: Si regex OU IA d√©tectent des PII -> WARNING = True
    final_warning = regex_warning or ai_warning
    
    logger.info(f"üéØ D√©cision finale: regex={regex_warning} OR ai={ai_warning} = {final_warning}")
    
    # 4. Construction du r√©sultat final
    if ai_result:
        result["summary"] = ai_result
        result["method_used"] = "ai_with_regex_fallback"
    else:
        result["summary"] = f"Analyse regex: {len(regex_pii)} types de PII d√©tect√©s: {', '.join(regex_pii)}"
        result["method_used"] = "regex_only"
    
    result["warning"] = final_warning
    result["pii_detected"] = regex_pii
    
    logger.info(f"‚úÖ Analyse termin√©e - Warning: {final_warning}")
    return result


def detect_pii_in_text(text: str) -> PIIDetectionResult:
    """D√©tection intelligente des informations personnelles avec IA"""
    try:
        result = get_agent().detect_pii_intelligent(text)
        logger.info(f"D√©tection PII: {result.pii_count} √©l√©ments trouv√©s, IA: {result.ai_analysis is not None}")
        return result
    except Exception as e:
        logger.error(f"Erreur lors de la d√©tection PII: {e}")
        raise

@mcp.tool()
def summarize_text(text: str, max_length: int = 150) -> SummaryResult:
    """R√©sum√© intelligent d'un texte avec IA"""
    try:
        result = get_agent().summarize_text(text, max_length)
        logger.info(f"R√©sum√© g√©n√©r√©: {result.summary_length} caract√®res, IA: {result.ai_enhanced}")
        return result
    except Exception as e:
        logger.error(f"Erreur lors du r√©sum√©: {e}")
        raise

@mcp.tool()
def translate_text(text: str, target_language: str = "en") -> TranslationResult:
    """Traduction intelligente d'un texte avec IA"""
    try:
        result = get_agent().translate_text(text, target_language)
        logger.info(f"Traduction vers {target_language} r√©ussie")
        return result
    except Exception as e:
        logger.error(f"Erreur lors de la traduction: {e}")
        raise

@mcp.tool()
def chat_with_agent(message: str) -> str:
    """Chat interactif avec l'agent IA NLP"""
    try:
        result = get_agent().chat(message)
        logger.info(f"Chat trait√©: {len(message)} caract√®res d'entr√©e")
        return result
    except Exception as e:
        logger.error(f"Erreur lors du chat: {e}")
        raise

@mcp.tool()
def reason_about_task(query: str) -> str:
    """Raisonnement et planification de t√¢ches avec l'agent IA"""
    try:
        result = get_agent().reason_and_act(query)
        logger.info(f"Raisonnement effectu√© pour: {query[:50]}...")
        return result
    except Exception as e:
        logger.error(f"Erreur lors du raisonnement: {e}")
        raise

@mcp.tool()
def process_multiple_files(file_paths: List[str]) -> BatchProcessingResult:
    """Traitement en lot de plusieurs files avec IA"""
    try:
        result = get_agent().process_multiple_files_with_reasoning(file_paths)
        logger.info(f"Traitement en lot termin√©: {len(file_paths)} files")
        
        return BatchProcessingResult(
            batch_info=result["batch_info"],
            files=result["files"],
            detailed_results=result["detailed_results"]
        )
    except Exception as e:
        logger.error(f"Erreur lors du traitement en lot: {e}")
        raise

@mcp.tool()
async def process_document(file_path: str, operations: List[str], ctx: Context) -> Dict[str, Any]:
    """Traitement complet d'un document avec plusieurs op√©rations utilisant l'IA"""
    await ctx.info(f"Traitement intelligent du document: {file_path}")
    
    try:
        # Lecture du fichier
        if file_path.lower().endswith('.pdf'):
            text = get_agent().extract_text_from_pdf(file_path)
            await ctx.info("üìÑ Texte extrait du PDF avec succ√®s")
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            await ctx.info("üìÑ Fichier texte lu avec succ√®s")
        
        results = {}
        agent_instance = get_agent()
        
        # Ex√©cution des op√©rations demand√©es avec IA
        for operation in operations:
            await ctx.info(f"ü§ñ Ex√©cution de l'op√©ration IA: {operation}")
            
            if operation == "analyze":
                results["analysis"] = agent_instance.analyze_text_basic(text)
                await ctx.info("‚úÖ Analyse compl√®te termin√©e")
            elif operation == "summarize":
                results["summary"] = agent_instance.summarize_text(text)
                await ctx.info("‚úÖ R√©sum√© intelligent g√©n√©r√©")
            elif operation == "detect_pii":
                results["pii_detection"] = agent_instance.detect_pii_intelligent(text)
                await ctx.info("‚úÖ D√©tection PII intelligente termin√©e")
            elif operation == "translate":
                results["translation"] = agent_instance.translate_text(text)
                await ctx.info("‚úÖ Traduction avec IA termin√©e")
            elif operation == "reason":
                reasoning = agent_instance.reason_and_act(f"Analyse ce document: {text[:500]}...")
                results["reasoning"] = {"analysis": reasoning}
                await ctx.info("‚úÖ Raisonnement IA termin√©")
            elif operation == "file_analysis":
                # Utiliser la m√©thode de traitement de fichier compl√®te
                file_result = agent_instance.process_file_with_reasoning(file_path)
                results["file_analysis"] = file_result
                await ctx.info("‚úÖ Analyse de fichier compl√®te termin√©e")
            else:
                await ctx.warning(f"Op√©ration inconnue: {operation}")
        
        await ctx.info("üéâ Traitement intelligent termin√© avec succ√®s")
        return results
        
    except Exception as e:
        await ctx.error(f"Erreur lors du traitement: {e}")
        raise

@mcp.tool()
async def batch_analyze_directory(directory_path: str, file_extensions: List[str], ctx: Context) -> BatchProcessingResult:
    """Analyse en lot d'un r√©pertoire complet avec IA"""
    await ctx.info(f"Analyse intelligente du r√©pertoire: {directory_path}")
    
    if not file_extensions:
        file_extensions = ['.txt', '.py', '.md', '.json', '.csv', '.xml', '.html', '.log', '.pdf']
    
    try:
        file_paths = []
        
        # Parcourir le r√©pertoire
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                file_path = os.path.join(root, file)
                _, ext = os.path.splitext(file.lower())
                
                if ext in file_extensions:
                    file_paths.append(file_path)
        
        await ctx.info(f"üìÅ {len(file_paths)} files trouv√©s")
        
        if not file_paths:
            await ctx.warning("Aucun fichier trouv√© avec les extensions sp√©cifi√©es")
            return BatchProcessingResult(
                batch_info={"error": "Aucun fichier trouv√©"},
                files=[],
                detailed_results={}
            )
        
        # Traitement en lot
        result = get_agent().process_multiple_files_with_reasoning(file_paths)
        await ctx.info(f"‚úÖ Traitement en lot termin√©: {len(file_paths)} files")
        
        return BatchProcessingResult(
            batch_info=result["batch_info"],
            files=result["files"],
            detailed_results=result["detailed_results"]
        )
        
    except Exception as e:
        await ctx.error(f"Erreur lors de l'analyse du r√©pertoire: {e}")
        raise

# Prompts MCP am√©lior√©s avec IA (identiques aux capacit√©s de l'agent original)
@mcp.prompt(title="Analyse Intelligente de Document")
def analyze_document_prompt(file_path: str, operations: str = "analyze,summarize,detect_pii") -> str:
    """Prompt pour l'analyse compl√®te d'un document avec IA"""
    ops_list = [op.strip() for op in operations.split(',')]
    return f"""
ü§ñ Agent IA NLP - Analyse Intelligente de Document

üìÑ Document √† analyser: {file_path}

üß† Capacit√©s IA disponibles:
- Llama 3.2 pour l'analyse contextuelle avanc√©e
- Raisonnement autonome (ReAct pattern)
- D√©tection PII intelligente avec compr√©hension du contexte
- R√©sum√©s adaptatifs selon le type de contenu
- Traduction contextuelle et culturellement appropri√©e
- Planification automatique des t√¢ches

üîß Op√©rations IA √† effectuer:
{chr(10).join(f'- {op} (avec intelligence artificielle)' for op in ops_list)}

üìä Formats support√©s:
- PDF (extraction intelligente avec PyMuPDF + PyPDF2)
- Texte (TXT, MD, JSON, CSV, XML, HTML, LOG, PY)

‚ö° Utiliser l'outil process_document avec les param√®tres appropri√©s.

L'agent utilisera son raisonnement autonome pour optimiser l'analyse selon le type de document.
"""

@mcp.prompt(title="D√©tection PII Intelligente")
def intelligent_pii_detection_prompt(text: str) -> str:
    """Prompt pour la d√©tection intelligente d'informations personnelles"""
    return f"""
üîí Agent IA NLP - D√©tection PII Intelligente

üìù Texte √† analyser: {text[:300]}...

üß† Analyse IA avanc√©e:
- Utilise Llama 3.2 pour l'analyse contextuelle
- Distinction automatique entre donn√©es r√©elles et exemples fictifs
- √âvaluation de la sensibilit√© selon le contexte
- D√©tection de patterns complexes et variations linguistiques

üîç Types de PII d√©tect√©s intelligemment:
- Adresses email r√©elles (filtrage des exemples)
- Num√©ros de t√©l√©phone avec validation contextuelle
- Informations financi√®res sensibles (cartes, IBAN, etc.)
- Donn√©es d'identit√© personnelle (SSN, passeports)
- Adresses et informations de localisation
- Noms et pr√©noms avec v√©rification contextuelle

‚öôÔ∏è M√©thodes de d√©tection:
1. Analyse IA contextuelle (primaire)
2. Patterns regex avanc√©s (fallback)
3. Validation crois√©e des r√©sultats

üéØ Utiliser l'outil detect_pii_in_text pour une analyse intelligente.

L'agent d√©terminera automatiquement le niveau de sensibilit√© des informations d√©tect√©es.
"""

@mcp.prompt(title="Raisonnement et Planification IA")
def reasoning_prompt(task: str) -> str:
    """Prompt pour le raisonnement et la planification de t√¢ches"""
    return f"""
üß† Agent IA NLP - Raisonnement Autonome

üéØ T√¢che √† analyser: {task}

ü§ñ Capacit√©s de raisonnement:
- Pattern ReAct (Reasoning + Acting)
- Planification multi-√©tapes
- Adaptation dynamique de la strat√©gie
- Utilisation autonome d'outils sp√©cialis√©s
- M√©moire conversationnelle
- Gestion d'erreurs intelligente

üîß Outils disponibles:
- Analyse de documents (PDF, texte, multi-formats)
- D√©tection PII avec compr√©hension contextuelle
- R√©sum√©s adaptatifs selon le type de contenu
- Traduction contextuelle
- Orchestration de workflows complexes
- Traitement en lot de files

‚ö° Capacit√©s avanc√©es:
- Analyse de documents multi-formats
- Batch processing intelligent
- D√©tection PII avec filtrage contextuel
- R√©sum√©s adaptatifs selon le domaine
- Traduction culturellement appropri√©e
- Planification automatique de t√¢ches

üéØ Utiliser l'outil reason_about_task pour d√©marrer l'analyse intelligente.

L'agent analysera la t√¢che et cr√©era un plan d'ex√©cution optimal.
"""

@mcp.prompt(title="Chat Interactif IA")
def chat_prompt(context: str = "") -> str:
    """Prompt pour le chat interactif avec l'agent IA"""
    return f"""
üí¨ Agent IA NLP - Chat Interactif

{f"üìã Contexte: {context}" if context else ""}

ü§ñ Capacit√©s conversationnelles:
- Raisonnement autonome avec Llama 3.2
- Compr√©hension contextuelle avanc√©e
- Planification automatique des actions
- Ex√©cution autonome de t√¢ches complexes
- M√©moire conversationnelle
- Adaptation dynamique selon les besoins

üí° Exemples d'interactions:
- "Analyse ce document et dis-moi s'il contient des informations sensibles"
- "R√©sume-moi les points cl√©s de ce texte en fran√ßais"
- "Y a-t-il des donn√©es personnelles dans ce fichier PDF ?"
- "Traduis ce texte en gardant le sens original et le contexte"
- "Traite tous les files de ce dossier et g√©n√®re un rapport"
- "Explique-moi pourquoi tu as d√©tect√© ces informations comme sensibles"

üîß Actions automatiques:
- Lecture et extraction de contenu (PDF, texte)
- Analyse intelligente et contextuelle
- D√©tection PII avec justification
- G√©n√©ration de r√©sum√©s adaptatifs
- Traduction contextuelle
- Sauvegarde automatique des r√©sultats

‚ö° Utiliser l'outil chat_with_agent pour commencer la conversation.

L'agent comprendra vos demandes et planifiera automatiquement les actions n√©cessaires.
"""

@mcp.prompt(title="Traitement en Lot")
def batch_processing_prompt(directory_or_files: str, operations: str = "analyze,detect_pii") -> str:
    """Prompt pour le traitement en lot de files"""
    ops_list = [op.strip() for op in operations.split(',')]
    return f"""
üîÑ Agent IA NLP - Traitement en Lot Intelligent

üìÅ Source: {directory_or_files}

üß† Capacit√©s de traitement en lot:
- Traitement parall√®le intelligent
- Priorisation automatique selon le type de fichier
- D√©tection de format automatique
- Gestion d'erreurs robuste
- G√©n√©ration de rapports consolid√©s

üîß Op√©rations IA √† effectuer:
{chr(10).join(f'- {op} (avec intelligence artificielle)' for op in ops_list)}

üìä Formats support√©s:
- Documents PDF (extraction intelligente)
- Fichiers texte (TXT, MD, JSON, CSV, XML, HTML, LOG)
- Code source (PY, JS, etc.)
- Traitement r√©cursif de dossiers

üìã R√©sultats g√©n√©r√©s:
- Rapport consolid√© avec statistiques
- Analyse individuelle par fichier
- D√©tection PII avec alertes
- R√©sum√©s intelligents par document
- Sauvegarde automatique des r√©sultats

‚ö° Utiliser l'outil process_multiple_files ou batch_analyze_directory selon le cas.

L'agent optimisera automatiquement le traitement selon le volume et le type de files.
"""

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Fonctions utilitaires compatibles avec l'agent original
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def process_file(file_path: str, offline_mode: bool = False) -> Dict[str, Any]:
    """Traite un fichier et retourne le r√©sultat JSON (compatible avec l'agent original)"""
    config_temp = NLPConfig()
    config_temp.offline_mode = offline_mode
    agent_temp = NLPAgent(config_temp)
    return agent_temp.process_file_with_reasoning(file_path)

def process_multiple_files_standalone(file_paths: List[str], offline_mode: bool = False) -> Dict[str, Any]:
    """Traite plusieurs files et retourne le r√©sultat JSON global (compatible avec l'agent original)"""
    config_temp = NLPConfig()
    config_temp.offline_mode = offline_mode
    agent_temp = NLPAgent(config_temp)
    return agent_temp.process_multiple_files_with_reasoning(file_paths)

def chat_mode():
    """Mode chat interactif avec l'agent IA (compatible avec l'agent original)"""
    print("ü§ñ Mode Chat Interactif - Agent IA NLP MCP")
    print("==========================================")
    print("Tapez 'quit' pour quitter, 'help' pour l'aide")
    
    agent_chat = get_agent()
    
    if not agent_chat.llm:
        print("‚ö†Ô∏è Agent IA non disponible, v√©rifiez Ollama")
        return
    
    print("\nüí¨ Chat pr√™t ! Posez vos questions...")
    
    while True:
        try:
            user_input = input("\nüë§ Vous: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("üëã Au revoir!")
                break
            
            if user_input.lower() == 'help':
                print("""
ü§ñ Commandes disponibles:
- Analysez un fichier: "Analyse le fichier recit.txt" ou "Analyse le fichier document.pdf"
- Listez les files: "Quels files sont disponibles?"
- Posez des questions: "R√©sume-moi ce document"
- D√©tection PII: "Y a-t-il des informations sensibles?"
- Traitement en lot: "Traite tous les files .txt du dossier"
- quit/exit: Quitter le chat

üìÑ Formats support√©s: .txt, .py, .md, .json, .csv, .xml, .html, .log, .pdf
ü§ñ L'agent utilise Llama 3.2 pour un raisonnement intelligent
                """)
                continue
            
            if not user_input:
                continue
            
            print("\nü§ñ Agent IA: ", end="")
            response = agent_chat.chat(user_input)
            print(response)
            
        except KeyboardInterrupt:
            print("\nüëã Au revoir!")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur: {e}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Interface principale MCP
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

if __name__ == "__main__":
    # Mode de lancement
    if len(sys.argv) > 1:
        if sys.argv[1] == "--chat":
            chat_mode()
        elif sys.argv[1] == "--test":
            # Test rapide de l'agent
            print("üîß Test de l'agent IA NLP MCP...")
            test_agent = get_agent()
            if test_agent.llm:
                print("‚úÖ Agent IA connect√© et pr√™t")
            else:
                print("‚ö†Ô∏è Agent IA en mode fallback")
            print("üéØ Utilisez --chat pour le mode interactif")
        else:
            print("‚ùå Options: --chat pour le mode interactif, --test pour tester")
    else:
        # Lancement du serveur MCP
        print("üöÄ D√©marrage de l'Agent IA NLP MCP...")
    print("=" * 50)
    print("[STARTUP] Agent IA autonome avec Llama 3.2")
    print("[STARTUP] Support multi-formats (PDF, texte, etc.)")
    print("[STARTUP] Detection PII intelligente")
    print("[STARTUP] Capacites de raisonnement et planification")
    print("[STARTUP] Traitement en lot et workflows complexes")
    print("=" * 50)
    print("[MCP] Serveur MCP disponible sur stdio")
    print("[MCP] Pret a recevoir des connexions MCP...")
    
    # V√©rification des capacit√©s
    if llm:
        print("[AI] Llama 3.2 connecte - Mode IA complet")
    else:
        print("[WARN] Llama non disponible - Mode fallback active")
    
    if PDF_AVAILABLE or PYMUPDF_AVAILABLE:
        print("[PDF] Support PDF disponible")
    else:
        print("[WARN] Support PDF limite")
        
        print("\n[INFO] Utilisez un client MCP pour vous connecter")
        print("[INFO] Ou lancez avec --chat pour le mode interactif")
        
        # Lancement du serveur MCP
        mcp.run()
