#!/usr/bin/env python3
"""
Interface Interactive RÃ©elle - Test avec Ollama/Llama
====================================================

Interface interactive complÃ¨te qui teste rÃ©ellement les agents MCP
avec intÃ©gration Ollama/Llama pour un fonctionnement authentique.
"""

import asyncio
import json
import os
import sys
import subprocess
import time
import logging
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RealMCPInterface:
    """Interface rÃ©elle pour tester le systÃ¨me MCP avec Ollama"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data" / "test_files"
        self.agents_dir = self.base_dir / "agents"
        self.logs_dir = self.base_dir / "logs"
        self.vault_dir = self.base_dir / "vault"
        
        # Configuration Ollama
        self.ollama_url = "http://localhost:11434"
        self.llama_model = "llama3.2:1b"
        
        logger.info("ğŸ¯ Interface MCP rÃ©elle initialisÃ©e")
    
    def check_ollama_available(self) -> bool:
        """VÃ©rifier si Ollama est disponible"""
        try:
            import requests
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                for model in models:
                    if self.llama_model in model.get('name', ''):
                        print(f"âœ… Ollama disponible avec {self.llama_model}")
                        return True
            print(f"âš ï¸ ModÃ¨le {self.llama_model} non trouvÃ©")
            return False
        except Exception as e:
            print(f"âŒ Ollama non disponible: {e}")
            return False
    
    async def query_ollama_direct(self, prompt: str) -> str:
        """RequÃªte directe Ã  Ollama"""
        try:
            import requests
            
            payload = {
                "model": self.llama_model,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'Aucune rÃ©ponse')
            else:
                return f"Erreur Ollama: {response.status_code}"
                
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    async def analyze_file_with_llama(self, file_path: str) -> Dict[str, Any]:
        """Analyser un fichier avec Llama"""
        try:
            # Lire le contenu du fichier
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # CrÃ©er un prompt d'analyse PII
            prompt = f"""
Analysez le texte suivant pour dÃ©tecter des informations personnelles identifiables (PII).

Recherchez:
- Noms et prÃ©noms
- Adresses email
- NumÃ©ros de tÃ©lÃ©phone
- Adresses postales
- NumÃ©ros de sÃ©curitÃ© sociale
- NumÃ©ros de carte bancaire
- Informations mÃ©dicales
- Informations confidentielles

Texte Ã  analyser:
{content[:2000]}

RÃ©pondez UNIQUEMENT par:
PII_DETECTED: OUI ou NON
CONFIDENCE: [0-100]%
TYPES: [liste des types trouvÃ©s]
SUMMARY: [rÃ©sumÃ© en une phrase]
"""
            
            print(f"ğŸ¤– Analyse IA en cours: {Path(file_path).name}")
            
            # RequÃªte Ã  Ollama
            response = await self.query_ollama_direct(prompt)
            
            # Parser la rÃ©ponse
            warning = "PII_DETECTED: OUI" in response.upper()
            
            return {
                "file_path": file_path,
                "summary": f"Analyse IA complÃ¨te: {response[:100]}...",
                "warning": warning,
                "agent_type": "nlp",
                "ai_response": response,
                "processing_time": 2.0
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur analyse Llama: {e}")
            return {
                "file_path": file_path,
                "summary": f"Erreur analyse IA: {str(e)}",
                "warning": False,
                "agent_type": "nlp",
                "processing_time": 0.0
            }
    
    async def run_agent_subprocess(self, agent_script: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ExÃ©cuter un agent via subprocess"""
        try:
            agent_path = self.agents_dir / agent_script
            if not agent_path.exists():
                return {"error": f"Agent script not found: {agent_script}"}
            
            # CrÃ©er un fichier temporaire pour l'input
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(input_data, f)
                temp_file = f.name
            
            try:
                # ExÃ©cuter l'agent
                result = subprocess.run(
                    [sys.executable, str(agent_path), temp_file],
                    capture_output=True,
                    text=True,
                    timeout=30,
                    cwd=str(agent_path.parent)
                )
                
                if result.returncode == 0:
                    # Parser la sortie JSON
                    try:
                        return json.loads(result.stdout)
                    except json.JSONDecodeError:
                        return {"output": result.stdout, "error": None}
                else:
                    return {"error": result.stderr, "output": result.stdout}
                    
            finally:
                # Nettoyer le fichier temporaire
                os.unlink(temp_file)
                
        except Exception as e:
            return {"error": str(e)}
    
    def print_interactive_menu(self):
        """Afficher le menu interactif"""
        print("\n" + "="*70)
        print("ğŸ¯ INTERFACE INTERACTIVE - SYSTÃˆME MCP RÃ‰EL")
        print("="*70)
        print("1. ğŸ§ª Tester un fichier avec Llama")
        print("2. ğŸ“ Analyser tout le rÃ©pertoire de test")
        print("3. ğŸ¤– Test direct agent NLP")
        print("4. ğŸ‘ï¸  Test direct agent Vision")
        print("5. ğŸ”Š Test direct agent Audio")
        print("6. ğŸ” Test direct agent Security")
        print("7. ğŸ“Š Test agent File Manager")
        print("8. ğŸ­ Simuler un workflow complet")
        print("9. ğŸ“‹ Voir le statut des agents")
        print("0. ğŸšª Quitter")
        print("="*70)
    
    async def test_single_file_with_llama(self):
        """Tester un fichier spÃ©cifique avec Llama"""
        files = list(self.data_dir.glob("*"))
        if not files:
            print("âŒ Aucun fichier de test trouvÃ©!")
            return
        
        print("\nğŸ“ Fichiers disponibles:")
        for i, file in enumerate(files, 1):
            if file.is_file():
                print(f"  {i}. {file.name}")
        
        try:
            choice = int(input("\nChoisir un fichier (numÃ©ro): "))
            if 1 <= choice <= len(files):
                file_path = files[choice - 1]
                
                if not self.check_ollama_available():
                    print("âŒ Ollama non disponible, test impossible")
                    return
                
                print(f"\nğŸ¤– Analyse avec Llama: {file_path.name}")
                result = await self.analyze_file_with_llama(str(file_path))
                
                print(f"\nğŸ“‹ RÃ©sultats:")
                print(f"  Fichier: {result['file_path']}")
                print(f"  Warning: {'ğŸ”´ OUI' if result['warning'] else 'ğŸŸ¢ NON'}")
                print(f"  RÃ©sumÃ©: {result['summary']}")
                if 'ai_response' in result:
                    print(f"\nğŸ¤– RÃ©ponse IA complÃ¨te:")
                    print(f"  {result['ai_response']}")
                    
            else:
                print("âŒ Choix invalide!")
                
        except ValueError:
            print("âŒ Veuillez entrer un numÃ©ro valide!")
    
    async def analyze_all_files(self):
        """Analyser tous les fichiers du rÃ©pertoire"""
        files = list(self.data_dir.glob("*.txt"))
        files.extend(list(self.data_dir.glob("*.json")))
        files.extend(list(self.data_dir.glob("*.csv")))
        files.extend(list(self.data_dir.glob("*.md")))
        
        if not files:
            print("âŒ Aucun fichier texte trouvÃ©!")
            return
        
        if not self.check_ollama_available():
            print("âŒ Ollama non disponible, analyse impossible")
            return
        
        print(f"\nğŸ” Analyse de {len(files)} fichiers...")
        
        results = []
        for file_path in files:
            print(f"\nğŸ“„ Traitement: {file_path.name}")
            result = await self.analyze_file_with_llama(str(file_path))
            results.append(result)
            
            status = "ğŸ”´ SENSIBLE" if result['warning'] else "ğŸŸ¢ SAFE"
            print(f"  Status: {status}")
        
        # RÃ©sumÃ©
        total = len(results)
        sensitive = sum(1 for r in results if r['warning'])
        safe = total - sensitive
        
        print(f"\nğŸ“Š RÃ©sumÃ© de l'analyse:")
        print(f"  â€¢ Total: {total} fichiers")
        print(f"  â€¢ Sensibles: {sensitive} fichiers")
        print(f"  â€¢ SÃ»rs: {safe} fichiers")
        
        if sensitive > 0:
            print(f"\nğŸ”´ Fichiers sensibles dÃ©tectÃ©s:")
            for result in results:
                if result['warning']:
                    print(f"  â€¢ {Path(result['file_path']).name}")
    
    async def simulate_full_workflow(self):
        """Simuler un workflow complet"""
        print("\nğŸ­ Simulation du workflow complet...")
        
        # Ã‰tape 1: Scan des fichiers
        files = list(self.data_dir.glob("*"))
        print(f"ğŸ“ Ã‰tape 1: Scan de {len(files)} fichiers")
        
        # Ã‰tape 2: Classification
        text_files = []
        image_files = []
        audio_files = []
        
        for file in files:
            if file.suffix.lower() in ['.txt', '.json', '.csv', '.md']:
                text_files.append(file)
            elif 'jpg' in file.name.lower() or 'png' in file.name.lower():
                image_files.append(file)
            elif 'mp3' in file.name.lower() or 'wav' in file.name.lower():
                audio_files.append(file)
        
        print(f"ğŸ“Š Classification:")
        print(f"  â€¢ Texte: {len(text_files)} fichiers")
        print(f"  â€¢ Image: {len(image_files)} fichiers") 
        print(f"  â€¢ Audio: {len(audio_files)} fichiers")
        
        # Ã‰tape 3: Traitement par agents
        all_results = []
        sensitive_files = []
        
        # Agent NLP
        if text_files and self.check_ollama_available():
            print(f"\nğŸ¤– Agent NLP: Traitement de {len(text_files)} fichiers texte...")
            for file in text_files[:3]:  # Limiter pour la dÃ©mo
                result = await self.analyze_file_with_llama(str(file))
                all_results.append(result)
                if result['warning']:
                    sensitive_files.append(str(file))
                print(f"  âœ… {file.name}: {'Sensible' if result['warning'] else 'Safe'}")
        
        # Agent Vision (simulation)
        if image_files:
            print(f"\nğŸ‘ï¸ Agent Vision: Traitement de {len(image_files)} fichiers image...")
            for file in image_files:
                is_sensitive = 'carte_identite' in file.name.lower()
                if is_sensitive:
                    sensitive_files.append(str(file))
                print(f"  âœ… {file.name}: {'Sensible' if is_sensitive else 'Safe'}")
        
        # Agent Audio (simulation)
        if audio_files:
            print(f"\nğŸ”Š Agent Audio: Traitement de {len(audio_files)} fichiers audio...")
            for file in audio_files:
                is_sensitive = 'reunion' in file.name.lower()
                if is_sensitive:
                    sensitive_files.append(str(file))
                print(f"  âœ… {file.name}: {'Sensible' if is_sensitive else 'Safe'}")
        
        # Ã‰tape 4: SÃ©curisation
        if sensitive_files:
            print(f"\nğŸ” Agent Security: SÃ©curisation de {len(sensitive_files)} fichiers sensibles...")
            for file_path in sensitive_files:
                encrypted_name = f"encrypted_{Path(file_path).name}.enc"
                vault_path = self.vault_dir / encrypted_name
                print(f"  ğŸ”’ {Path(file_path).name} â†’ {encrypted_name}")
        
        # Ã‰tape 5: Rapport final
        print(f"\nğŸ“‹ Rapport final du workflow:")
        print(f"  â€¢ Fichiers traitÃ©s: {len(files)}")
        print(f"  â€¢ Fichiers sensibles: {len(sensitive_files)}")
        print(f"  â€¢ Fichiers sÃ©curisÃ©s: {len(sensitive_files)}")
        print(f"  â€¢ Fichiers sÃ»rs: {len(files) - len(sensitive_files)}")
        
        print(f"\nğŸ‰ Workflow terminÃ© avec succÃ¨s!")
    
    def show_agent_status(self):
        """Afficher le statut des agents"""
        print("\nğŸ“‹ Statut des agents MCP:")
        
        agents = [
            ("orchestrator", 8001),
            ("nlp", 8002),
            ("vision", 8003),
            ("audio", 8004),
            ("file_manager", 8005),
            ("security", 8006)
        ]
        
        for agent_name, port in agents:
            script_path = self.agents_dir / f"agent_{agent_name}_mcp.py"
            status = "âœ… Disponible" if script_path.exists() else "âŒ Non trouvÃ©"
            print(f"  â€¢ {agent_name:<15} (:{port}) - {status}")
        
        # VÃ©rifier Ollama
        ollama_status = "âœ… ConnectÃ©" if self.check_ollama_available() else "âŒ Indisponible"
        print(f"  â€¢ Ollama/Llama       - {ollama_status}")
    
    async def run_interactive_interface(self):
        """ExÃ©cuter l'interface interactive"""
        print("\nğŸ¯ Bienvenue dans l'interface de test MCP rÃ©elle!")
        print("Cette interface vous permet de tester le systÃ¨me avec de vrais agents.")
        
        while True:
            self.print_interactive_menu()
            
            try:
                choice = input("\nVotre choix: ").strip()
                
                if choice == "1":
                    await self.test_single_file_with_llama()
                
                elif choice == "2":
                    await self.analyze_all_files()
                
                elif choice == "3":
                    print("\nğŸ¤– Test agent NLP - FonctionnalitÃ© en dÃ©veloppement")
                
                elif choice == "4":
                    print("\nğŸ‘ï¸ Test agent Vision - FonctionnalitÃ© en dÃ©veloppement")
                
                elif choice == "5":
                    print("\nğŸ”Š Test agent Audio - FonctionnalitÃ© en dÃ©veloppement")
                
                elif choice == "6":
                    print("\nğŸ” Test agent Security - FonctionnalitÃ© en dÃ©veloppement")
                
                elif choice == "7":
                    print("\nğŸ“Š Test agent File Manager - FonctionnalitÃ© en dÃ©veloppement")
                
                elif choice == "8":
                    await self.simulate_full_workflow()
                
                elif choice == "9":
                    self.show_agent_status()
                
                elif choice == "0":
                    print("\nğŸ‘‹ Au revoir!")
                    break
                
                else:
                    print("âŒ Choix invalide!")
                
                input("\nAppuyez sur EntrÃ©e pour continuer...")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Au revoir!")
                break

async def main():
    """Point d'entrÃ©e principal"""
    interface = RealMCPInterface()
    await interface.run_interactive_interface()

if __name__ == "__main__":
    asyncio.run(main())
